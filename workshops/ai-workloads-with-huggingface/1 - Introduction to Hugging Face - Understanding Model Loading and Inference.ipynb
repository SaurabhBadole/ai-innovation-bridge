{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee762827-64ac-4764-b228-06af4ecb98cb",
   "metadata": {},
   "source": [
    "# Introduction to Hugging Face: Understanding Model Loading and Inference\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.png\" alt=\"Alt Text\" style=\"width: 400px;\"/>\r\n",
    "\r\n",
    "\r\n",
    "Welcome to our workshop on the fundamentals of Hugging Face, a pivotal platform in the world of Natural Language Processing (NLP) and Machine Learning (ML). This session is designed to guide you through the basic mechanics of Hugging Face, particularly focusing on loading models and performing inference. We will explore both direct methods using the `generate` function from loaded models and the streamlined approach offered by Hugging Face pipelines.\r\n",
    "\r\n",
    "## What is Hugging Face?\r\n",
    "\r\n",
    "Hugging Face is a revolutionary platform in the AI community, best known for its Transformers library. It provides an extensive collection of pre-trained models that are crucial for a variety of NLP tasks such as text classification, question answering, text generation, and more. These models, built on the transformer architecture, have transformed the way we approach language understanding and generation in AI.\r\n",
    "\r\n",
    "## The Value of Hugging Face\r\n",
    "\r\n",
    "The true value of Hugging Face lies in its accessibility and ease of use. With just a few lines of code, developers can leverage complex, state-of-the-art models for their NLP needs. This ease of access democratizes cutting-edge AI technology, making it available to a broader range of users and developers. Whether it's fine-tuning models on specific datasets or quickly deploying pre-trained models for inference, Hugging Face simplifies these processes remarkably.\r\n",
    "\r\n",
    "## Why Learn Hugging Face?\r\n",
    "\r\n",
    "For developers venturing into NLP and ML, Hugging Face is an indispensable tool. It not only accelerates the development process but also provides a platform for experimentation and learning. Understanding how to effectively load models and perform inference is crucial for building robust AI applications. As the field of NLP continues to evolve, proficiency in Hugging Face will become increasingly valuable.\r\n",
    "\r\n",
    "## Foundation for the Workshop\r\n",
    "\r\n",
    "The skills and concepts we develop here form the foundation for more advanced topics in our workshop. By mastering model loading and inference, you'll be well-prepared to tackle more complex tasks, experiment with different models, and appreciate the nuances of model performance and adaptation. Let's embark on this journey to unlock the full potential of Hugging Face in AI development.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af370f22-3335-4ad7-a049-08445f79e37b",
   "metadata": {},
   "source": [
    "#### Environment Setup and Dependency Installation\n",
    "\n",
    "This cell is crucial for setting up the environment and installing necessary dependencies. We install specific versions of the `transformers` and `torch` libraries. These libraries are essential for leveraging the full capabilities of Intel hardware and optimizing model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175e5202-27ad-47a8-9906-e831ad51db6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ":: WARNING: setvars.sh has already been run. Skipping re-execution.\n",
      "   To force a re-execution of setvars.sh, use the '--force' option.\n",
      "   Using '--force' can result in excessive use of your environment variables.\n",
      "  \n",
      "usage: source setvars.sh [--force] [--config=file] [--help] [...]\n",
      "  --force        Force setvars.sh to re-run, doing so may overload environment.\n",
      "  --config=file  Customize env vars using a setvars.sh configuration file.\n",
      "  --help         Display this help message and exit.\n",
      "  ...            Additional args are passed to individual env/vars.sh scripts\n",
      "                 and should follow this script's arguments.\n",
      "  \n",
      "  Some POSIX shells do not accept command-line options. In that case, you can pass\n",
      "  command-line options via the SETVARS_ARGS environment variable. For example:\n",
      "  \n",
      "  $ SETVARS_ARGS=\"ia32 --config=config.txt\" ; export SETVARS_ARGS\n",
      "  $ . path/to/setvars.sh\n",
      "  \n",
      "  The SETVARS_ARGS environment variable is cleared on exiting setvars.sh.\n",
      "  \n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-528k1dlc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-528k1dlc\n",
      "  Resolved https://github.com/huggingface/transformers to commit 5c8d941d66734811d2ef6f57f15b44f7fb7a98c4\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.0.dev0)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from transformers==4.38.0.dev0) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.8.0)\n",
      "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.0.dev0)\n",
      "  Downloading tokenizers-0.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers==4.38.0.dev0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers==4.38.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from requests->transformers==4.38.0.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/python3.9/lib/python3.9/site-packages (from requests->transformers==4.38.0.dev0) (2023.7.22)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.38.0.dev0-py3-none-any.whl size=8439520 sha256=f34530b17714789df7a215d19249bcfcf04d5dcd81fe83805b148037de286dc6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-w7tx9t5z/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b\n",
      "Successfully built transformers\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/home/uad6b15e0ae3d5e407195ab5f044a50f/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "intel-extension-for-transformers 1.2.2 requires transformers==4.34.1, but you have transformers 4.38.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.20.3 tokenizers-0.15.1 transformers-4.38.0.dev0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.1.0 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from torch==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0) (12.3.52)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from jinja2->torch==2.1.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/uad6b15e0ae3d5e407195ab5f044a50f/.local/lib/python3.9/site-packages (from sympy->torch==2.1.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!source /opt/intel/oneapi/setvars.sh #comment out if not running on Intel Developer Cloud Jupyter\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install torch==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590e167-5e1f-4a03-85a7-88e86bae7c40",
   "metadata": {},
   "source": [
    "#### Model Loading\r\n",
    "Here, we load the model and tokenizer from Hugging Face. We're using `AutoModelForCausalLM` and `AutoTokenizer` from the `transformers` library to load \"stabilityai/stablelm-2-1_6b\", a causal language model. The `trust_remote_code` flag is set to `True` for remote code execution, and `torch_dtype` is set to \"auto\" for automatic precision setting, optimizing performance on the underlying hardware..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0d006f-248d-4404-bbca-b7910c869406",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-2-1_6b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  \"stabilityai/stablelm-2-1_6b\",\n",
    "  trust_remote_code=True,\n",
    "  torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46f252-775c-4b12-a454-321ac8f801b0",
   "metadata": {},
   "source": [
    "#### Generating Text\r\n",
    "In this cell, we perform text generation. We first tokenize a prompt (\"I love learning to code...\") and then use the `generate` method of our model to create new text. The generation parameters like `max_new_tokens`, `temperature`, and `top_p` are configured to control the creativity and coherence of the generated text. The result is then decoded and printed, showing the model's response to our prompt.\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368a3e1d-5256-4c42-967f-eb28d8460396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love learning to code... and I love being able to teach others how to code. I've been teaching people how to code for almost 2 years now. The best part is that it's completely free and you don't need to have any previous programming experience to start learning.\n",
      "If you're interested in learning to code, I've created a\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"I love learning to code...\", return_tensors=\"pt\").to(model.device)\n",
    "tokens = model.generate(\n",
    "  **inputs,\n",
    "  max_new_tokens=64,\n",
    "  temperature=0.70,\n",
    "  top_p=0.95,\n",
    "  do_sample=True,\n",
    ")\n",
    "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128e45f-b309-41b2-9ebe-90bb699e1eff",
   "metadata": {},
   "source": [
    "#### Setting Up a Pipeline for Text Generation\r\n",
    "This cell demonstrates setting up a Hugging Face pipeline for text generation. We initialize a `pipeline` object with our model and tokenizer, configuring it specifically for text generation. Parameters like `torch_dtype` for precision and `device_map` for device allocation are set for optimal performance. We then use this pipeline to generate a sequence from a given prompt, showcasing an alternative, more streamlined approach to model inference..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6054b5af-29c9-471f-acf2-7df0681fa315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:100257 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "sequences = generator( \n",
    "            \"I love learning to code...\",\n",
    "            max_length=64,\n",
    "            do_sample=True,\n",
    "            top_k=3,\n",
    "            temperature=0.70,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac23be6-1f97-4a51-8f0d-c38b48200539",
   "metadata": {},
   "source": [
    "#### Displaying Pipeline Results\r\n",
    "In the final cell, we iterate through the sequences generated by our pipeline and print them out. This step is crucial for visualizing the outputs of our text generation pipeline, allowing us to evaluate the model's performance in generating coherent and contextually relevant text..\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8973235f-52bf-47d9-8867-648ba80064d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I love learning to code... but I'm not a programmer. I'm not a designer. I'm not a developer. I'm not a business owner. I'm not a marketer. I'm not a salesperson. I'm not a writer. I'm not a manager.\n",
      "I'm a marketer. I'm\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "         print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04dbe7c-67b4-42bf-a48e-11bec5a8898b",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Throughout this workshop, we've explored the basic mechanics of using Hugging Face, focusing on loading models, performing inference, and utilizing pipelines for efficient text generation. We've leveraged the power of Intel hardware optimizations and the versatility of Hugging Face's Transformers library to achieve high-performance model inference.\r\n",
    "\n",
    "#### Discussion\n",
    "\n",
    "The exercises demonstrated not only the ease of using pre-trained models from Hugging Face but also the importance of understanding the underlying hardware optimizations. Developers should now have a foundational understanding of model loading, text generation, and the utilization of pipelines in Hugging Face, which are crucial for developing efficient NLP applications. This knowledge serves as a stepping stone to more advanced topics in AI and ML, enabling developers to harness the full potential of modern NLP technologies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
