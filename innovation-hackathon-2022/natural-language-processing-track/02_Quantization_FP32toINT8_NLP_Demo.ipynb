{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b7ff3a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<center><img src=\"../pngs/20220926_hackathon_nlp_track.png\" style=\"border-radius:15px\"></center>\n",
    "\n",
    "# <b><div style=\"color:#211894;font-size:100%;text-align:center\">Welcome to the NLP Track of the Hackathon! Part 2 of 2: Quantization üöÄ</div></b>\n",
    "\n",
    "<h1><center>Author: Benjamin Consolvo <br></center></h1>\n",
    "\n",
    "# <a id=\"TOC\">Table of Contents</a> \n",
    "- [1. Introduction: Problem Statement & Dataset, Model Architecture, Hardware, and Software](#introduction)  \n",
    "- [2. Importing of Libraries](#install)  \n",
    "- [3. Data Loading](#data)  \n",
    "- [4. Quantization](#quant)\n",
    "- [5. Model Inference on Intel Gen. 3 Xeon CPU](#inference)\n",
    "- [6. MLFlow Model Registration](#mlflowreg)\n",
    "- [7. Summary](#summary)\n",
    "- [8. References](#references)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f78f95",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #dc98ff, #251cab)\">1. Introduction: Problem Statement, Model Architecture, Hardware, and Software</div>\n",
    "\n",
    "Hello and welcome to this NLP quantization notebook! I first am going to briefly spend some time introducing the problem, the model architecture, the hardware, and the software we will be using in two companion notebooks. During the demo, I have already run all the cells so that you are not sitting and waiting for anything, but you are of course encouraged to run them yourself and to change as much as you‚Äôd like.\n",
    "\n",
    "I made a video introduction to the notebook here: https://www.intel.com/content/www/us/en/developer/videos/ai-for-social-good-hackathon.html\n",
    "\n",
    "Timestamps:\n",
    "#### Notebook 1 of 2 - Habana Training Demo\n",
    "- 0:00 - 1. Introduction  \n",
    "    - 0:25 - Problem Statement\n",
    "    - 1:25 - Model Architecture - DistilBERT\n",
    "    - 1:50 - Hardware - Habana¬Æ Gaudi¬Æ HPU and 3rd Generation Intel¬Æ Xeon¬Æ\n",
    "    - 3:15 - Monitoring compute\n",
    "    - 3:55 - Software \n",
    "- 5:10 - 2. Importing of Libraries\n",
    "- 6:13 - 3. Exploratory Data Analysis (EDA) and Tokenization\n",
    "    - 7:27 - Tokenization\n",
    "    - 9:20 - Histogram and word cloud\n",
    "    - 11:06 - `torch.tensor` format\n",
    "- 11:54 - 4. Model Training on Habana¬Æ Gaudi¬Æ HPU \n",
    "    - 11:54 - Setting up training\n",
    "    - 13:42 - Training the model\n",
    "- 15:09 - 6. Model Performance and Sample Inference\n",
    "    - 15:19 - Inference on unseen test dataset\n",
    "    - 15:58 - Inference on Single Sample\n",
    "- 17:16 - 8. Summary \n",
    "- 18:39 - 9. References\n",
    "\n",
    "#### Notebook 2 of 2 - Quantization Demo\n",
    "- 19:03 - 1. Introduction\n",
    "- 19:32 - 2. Importing of Libraries\n",
    "- 20:08 - 3. Data Loading\n",
    "- 20:22 - 4. Quantization\n",
    "- 22:46 - 5. Model Inference on Intel Gen. 3 Xeon CPU\n",
    "    - 23:03 - FP32 model\n",
    "    - 23:26 - INT8 model\n",
    "- 23:54 - 6. Summary\n",
    "- 24:20 - END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac745ec-cd44-479c-b0f8-9217deea2fc8",
   "metadata": {},
   "source": [
    "#### Problem Statement\n",
    "In a world where negativity in speech and media is prominent, humor can help uplift the human spirit. ‚ÄúHow to create a method or model to discover the structures behind humor, recognize humor ‚Ä¶ remains a challenge because of its subjective nature‚Äù ([Jain, 2017](https://core.ac.uk/download/pdf/234824434.pdf)). Machine learning and deep learning has been progressing to produce powerful language models. The proposed challenge here is to teach a computer how to distinguish between an humorous and non-humorous statement in English.\n",
    "\n",
    "In the first of two Jupyter notebooks, we will train a binary text classification model to determine if a statement is humorous. For the demonstration, I will only use a small portion of the data for training. \n",
    "- You are encouraged to use more/all of the data to improve the efficacy of your model.\n",
    "- You are also encouraged to experiment with data tokenization, preprocessing, and augmentation.\n",
    "\n",
    "In the second Jupyter notebook, we will increase the performance of prediction (or inference) in a simulated production environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61efe02-ba0d-40f5-9ea1-35d8406a4e56",
   "metadata": {},
   "source": [
    "#### Model Architecture\n",
    "- Today, we will use a distilled version of the BERT transformer-based model architecture, called DistilBERT (https://arxiv.org/abs/1910.01108). You can find a description of the model on Huggingface here: https://huggingface.co/distilbert-base-uncased. It is a smaller, faster, distilled version of BERT.   You are free and encouraged to experiment with other architectures.   \n",
    "- BERT stands for Bidirectional Encoder Representations for Transformers, and it is a deep learning model for natural language processing (NLP) that can be used for a variety of language tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642fceea-0717-40ee-8d6f-d888092c175a",
   "metadata": {},
   "source": [
    "#### Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a456c-e6b7-431b-99d0-e040714d3522",
   "metadata": {},
   "source": [
    "##### Habana¬Æ Gaudi¬Æ HPU\n",
    "- For the first notebook, we will be training our model using a Habana¬Æ Gaudi¬Æ HPU (Habana Processing Unit) accelerator, hosted on AWS. The instance is an Amazon EC2 dl1.24xlarge (https://aws.amazon.com/ec2/instance-types/dl1/). It is an 8x parallel accelerator (HPU) that beats comparable GPU-based instances \"by up to 40%\" and at a much-reduced cost (https://habana.ai/training/gaudi/). \n",
    "\n",
    "- Due to the smaller size of the dataset and relatively low training time, I am only covering single-HPU training here, but if you would like to try distributed training over multiple HPUs, you can visit the Optimum Habana GitHub repository to learn from the examples of distributed training there (https://github.com/huggingface/optimum-habana/tree/main/examples/text-classification).\n",
    "\n",
    "- The Habana¬Æ Gaudi¬Æ DL1 instances come with 96 2nd Generation Intel¬Æ Xeon¬Æ vCPUs (48 physical cores)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147d5bf-40f8-4b56-a189-9954dc0a1ffb",
   "metadata": {},
   "source": [
    "##### 3rd Generation Intel¬Æ Xeon¬Æ Platinum 8375C Ice Lake CPU\n",
    "- In the second notebook, I will be showing you how to speed up inference time using a technique called ‚Äúquantization‚Äù on a production-capable 3rd Generation Intel¬Æ Xeon¬Æ Platinum 8375C Ice Lake CPU (https://ark.intel.com/content/www/us/en/ark/products/series/204098/3rd-generation-intel-xeon-scalable-processors.html). This instance is called m6i.*xlarge on AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889f6a5-4bc3-408b-be4e-c68c5bb3ee8d",
   "metadata": {},
   "source": [
    "##### 4rd Generation Intel¬Æ Xeon¬Æ Sapphire Rapids CPU\n",
    "\n",
    "- Intel will be releasing a 4th Generation Xeon¬Æ Sapphire Rapids CPU processor with Advanced Matrix Extension that will be able to offer a performance speed improvement for inference of up to 8X on INT8 model as compared to INT8 on the 3rd Generation Xeon¬Æ Ice Lake CPU. For more information about the upcoming performance benefits, you can visit:\n",
    "    - https://edc.intel.com/content/www/us/en/products/performance/benchmarks/architecture-day-2021/?r=1156525610"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459564b9-052e-4146-a90c-05bbc6cf7448",
   "metadata": {},
   "source": [
    "##### Pro tips for monitoring compute\n",
    "- To actively monitor the compute on the HPUs, you can use `watch hl-smi`, similar to `watch nvidia-smi` on NVIDIA GPUs.\n",
    "- To monitor the compute on the CPU cores and memory usage, you can use `htop` in the command line. And to get a printout of the CPU information, you can use a command called `lscpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d855a492-db19-4ec9-b919-bcf0732ed382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:            x86_64\n",
      "  CPU op-mode(s):        32-bit, 64-bit\n",
      "  Address sizes:         46 bits physical, 48 bits virtual\n",
      "  Byte Order:            Little Endian\n",
      "CPU(s):                  16\n",
      "  On-line CPU(s) list:   0-15\n",
      "Vendor ID:               GenuineIntel\n",
      "  Model name:            Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz\n",
      "    CPU family:          6\n",
      "    Model:               106\n",
      "    Thread(s) per core:  2\n",
      "    Core(s) per socket:  8\n",
      "    Socket(s):           1\n",
      "    Stepping:            6\n",
      "    BogoMIPS:            5799.92\n",
      "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\n",
      "                         a cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscal\n",
      "                         l nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopo\n",
      "                         logy nonstop_tsc cpuid aperfmperf tsc_known_freq pni pc\n",
      "                         lmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe \n",
      "                         popcnt tsc_deadline_timer aes xsave avx f16c rdrand hyp\n",
      "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd i\n",
      "                         brs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 a\n",
      "                         vx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx \n",
      "                         smap avx512ifma clflushopt clwb avx512cd sha_ni avx512b\n",
      "                         w avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd ida \n",
      "                         arat avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmu\n",
      "                         lqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdp\n",
      "                         id md_clear flush_l1d arch_capabilities\n",
      "Virtualization features: \n",
      "  Hypervisor vendor:     KVM\n",
      "  Virtualization type:   full\n",
      "Caches (sum of all):     \n",
      "  L1d:                   384 KiB (8 instances)\n",
      "  L1i:                   256 KiB (8 instances)\n",
      "  L2:                    10 MiB (8 instances)\n",
      "  L3:                    54 MiB (1 instance)\n",
      "NUMA:                    \n",
      "  NUMA node(s):          1\n",
      "  NUMA node0 CPU(s):     0-15\n",
      "Vulnerabilities:         \n",
      "  Itlb multihit:         Not affected\n",
      "  L1tf:                  Not affected\n",
      "  Mds:                   Not affected\n",
      "  Meltdown:              Not affected\n",
      "  Mmio stale data:       Vulnerable: Clear CPU buffers attempted, no microcode; \n",
      "                         SMT Host state unknown\n",
      "  Retbleed:              Not affected\n",
      "  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl\n",
      "                          and seccomp\n",
      "  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer\n",
      "                          sanitization\n",
      "  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB fillin\n",
      "                         g\n",
      "  Srbds:                 Not affected\n",
      "  Tsx async abort:       Not affected\n"
     ]
    }
   ],
   "source": [
    "!lscpu #cpu information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf30c2-30ef-456b-9b3c-5c3cf047f0fe",
   "metadata": {},
   "source": [
    "Though we are using these specific hardware architectures, I have attempted to make the code as accessible as possible by offering alternative code in the notebooks for other hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d421f-6789-4bea-874f-c7a8f1259ead",
   "metadata": {},
   "source": [
    "#### Software\n",
    "\n",
    "I will now briefly highlight some of the key Python libraries I will be using in the two notebooks. \n",
    "- We will be using the Habana SynapseAI fork of PyTorch. It looks and feels much like the stock PyTorch, but it has been optimized for Habana¬Æ Gaudi¬Æ HPUs.\n",
    "- Stock CPU PyTorch (https://pytorch.org/get-started/locally/) for inference.\n",
    "- The Huggingface ü§ó `transformers` library is what we are using to pull our DistilBERT pre-trained model from and the associated configuration prior to training.\n",
    "- For setting up the training, we will be using `optimum.habana`, which is ‚Äúthe interface between the Transformers library and Habana‚Äôs¬Æ Gaudi¬Æ HPU‚Äù (https://github.com/huggingface/optimum-habana).  \n",
    "- To speed up model inference, we will be using `optimum.intel`, which is ‚Äúthe interface between the Transformers library and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures.‚Äù (https://github.com/huggingface/optimum-intel). In particular, the Intel Neural Compressor (INC) is used in the backend for quantization of a model from FP32 to INT8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac36575c-3231-4266-a1a4-cc37acb61958",
   "metadata": {},
   "source": [
    "## <span style=\"color:#211894;font-size:100%;text-align:left\">Evaluation Guidelines</span>\n",
    "\n",
    "#### Judging criteria:\n",
    "    - 70% F1 score\n",
    "    - 30% Inference speed\n",
    "Make your model as fast as possible during inference, while retaining a high F1 score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2611b9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"install\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #dc98ff, #251cab)\">2. Importing of Libraries</div>\n",
    "We now move onto the second main section: ‚ÄúImporting of Libraries‚Äù.\n",
    "\n",
    "Before importing tools, I just run these couple of lines starting with `%load_ext autoreload` to automatically reload any updated local Python libraries into the Jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97f4bf5-db72-44dc-a43c-cbdaee7b72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lines to automatically reload any new local libraries as they are updated.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab0590",
   "metadata": {},
   "source": [
    "Instead of using the SynapseAI PyTorch, we will now use the CPU version of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc7e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/nlp_hackathon_inference/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Importing libraries \n",
    "import torch\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append('./src/')\n",
    "import nlpload, evaluate\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    DistilBertConfig, \n",
    "    AutoConfig, \n",
    "    DistilBertTokenizerFast, \n",
    "    DistilBertForSequenceClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EvalPrediction, \n",
    "    default_data_collator\n",
    ")\n",
    "import mlflow\n",
    "import time\n",
    "import numpy as np\n",
    "import boto3\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0a434e-5225-4333-8ac1-55390bcf65de",
   "metadata": {},
   "source": [
    "Also of note here, we are loading two libraries that we need for quantization: `neural_compressor` and `optimum.intel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc504a48-d6be-407d-8c64-329ed36f2c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##These libraries are necessary to run quantization \n",
    "import neural_compressor\n",
    "from optimum.intel.neural_compressor import (\n",
    "    IncDistillationConfig, \n",
    "    IncDistiller,\n",
    "    IncOptimizer,\n",
    "    IncPruner,\n",
    "    IncPruningConfig,\n",
    "    IncQuantizationConfig,\n",
    "    IncQuantizationMode,\n",
    "    IncQuantizer,\n",
    "    IncTrainer\n",
    ")\n",
    "from optimum.intel.neural_compressor.quantization import IncQuantizedModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576357a-5a88-4581-9ba0-4b888b3afbc6",
   "metadata": {},
   "source": [
    "And I am setting the `torch` device to `cpu` instead of `hpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66aa7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('gpu') #NVIDIA-GPU\n",
    "device = torch.device('cpu') #CPU-only\n",
    "# device = torch.device('mps') #Mac M1/M2 GPU\n",
    "# device = torch.device('hpu') #Habana HPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51720fa7",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #dc98ff, #251cab)\">3. Data Loading</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8844c75-df37-49f4-b287-5c263650a51d",
   "metadata": {},
   "source": [
    "I have simplified the data loading only to the essentials with no data exploration here, but I am taking the same steps as I did in the first notebook to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "354e7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "hdf = pd.read_csv('./data/dataset.csv')\n",
    "hdf[\"label\"] = hdf[\"humor\"].astype(int) #convert True/False into a 0 or a 1\n",
    "hdf2 = hdf.sample(frac=0.10).reset_index(drop=True)\n",
    "train, val, test = np.split(hdf2.sample(frac=1, random_state=42),  #train val test split\n",
    "                       [int(.9*len(hdf2)), int(.95*len(hdf2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf25e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(list(train['text']), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val['text']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test['text']), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5158c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class newDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16427d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = newDataset(train_encodings, list(train['label']))\n",
    "val_dataset = newDataset(val_encodings, list(val['label']))\n",
    "test_dataset = newDataset(test_encodings, list(test['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e0e12-c2cc-48f2-9538-7c14148d0de1",
   "metadata": {},
   "source": [
    "If you have split up the data into a test set, you can load it in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf2044a-4136-428c-a2e8-cb5678074853",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvpath = './data/dataset.csv'\n",
    "batch_size = 1000\n",
    "torch_dataloader = nlpload.mainLoader(csvpath,batch_size,labels=False) #function in ../src/nlpload.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d39a0",
   "metadata": {},
   "source": [
    "<a id=\"quant\"></a>\n",
    "\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #dc98ff, #251cab)\">4. Quantization</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72afccee-2a66-4e3a-b238-cb41d783e2fe",
   "metadata": {},
   "source": [
    "We now turn to quantization. To save time at inference, we can quantize the model from FP32 to INT8, without much drop in accuracy. This step is not necessary to submit your model, but it should save on inference time. \n",
    "\n",
    "To learn more about the functions and quantization, you can visit the `optimum.intel` GitHub repository with text classification examples (https://github.com/huggingface/optimum-intel/tree/main/examples/neural_compressor/text-classification). This is where I learned how to apply quantization and present it to you here in this notebook.\n",
    "\n",
    "First, we need to load the previously trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eaa579-49ee-4019-be95-cde72989d4ca",
   "metadata": {},
   "source": [
    "I then set up the trainer, but this time use the ‚ÄúIncTrainer‚Äù, or the Intel Neural Compressor Trainer class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07875f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading the model\n",
    "output_model_folder = './models/checkpoint-2000' #this may change depending on where you saved your model.\n",
    "model_fp32 = DistilBertForSequenceClassification.from_pretrained(output_model_folder) \n",
    "model_fp32.to(device)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e671ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output_quantized\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = IncTrainer(\n",
    "        model=model_fp32,\n",
    "        args=training_args, \n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9284d5f-4413-43d8-a666-5e5571833935",
   "metadata": {},
   "source": [
    "We can then run a baseline model for inference using the FP32 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cd6d38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 650\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 16:22:57 [INFO] eval_accuracy: 0.9769230769230769\n",
      "2022-09-24 16:22:57 [INFO] Throughput: 234.266 samples/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "metric_name = \"eval_accuracy\"\n",
    "def take_eval_steps(model, trainer, metric_name, save_metrics=False):\n",
    "    trainer.model = model\n",
    "    metrics = trainer.evaluate()\n",
    "    if save_metrics:\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "    logger.info(\"{}: {}\".format(metric_name, metrics.get(metric_name)))\n",
    "    logger.info(\"Throughput: {} samples/sec\".format(metrics.get(\"eval_samples_per_second\")))\n",
    "    return metrics[metric_name]\n",
    "\n",
    "result_baseline_model = take_eval_steps(model_fp32, trainer, metric_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e775e-7c12-4a53-999c-e776c8900fac",
   "metadata": {},
   "source": [
    "We now set everything up that we need to run quantization. One thing I do want to point out is that I am using a configuration file called `quantization.yml` that I downloaded from the `optimum.intel` GitHub page (https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/config/quantization.yml). You can adjust some of these parameters if you would like to adjust how the model is quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59dfdb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_train_steps(model, trainer, agent=None, resume_from_checkpoint=None, last_checkpoint=None):\n",
    "    trainer.model_wrapped = model\n",
    "    trainer.model = model\n",
    "    train_result = trainer.train(agent)\n",
    "    metrics = train_result.metrics\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    return trainer.model\n",
    "\n",
    "def train_func(model):\n",
    "    # return take_train_steps(model, trainer, resume_from_checkpoint, last_checkpoint)\n",
    "    return take_train_steps(model, trainer)\n",
    "\n",
    "def eval_func(model):\n",
    "    return take_eval_steps(model, trainer, metric_name)\n",
    "\n",
    "\n",
    "q8_config = IncQuantizationConfig.from_pretrained(\n",
    "            config_name_or_path = './config/quantization.yml' #from https://github.com/huggingface/optimum-intel/blob/main/examples/neural_compressor/config/quantization.yml\n",
    "        )\n",
    "\n",
    "quant_approach = IncQuantizationMode(q8_config.get_config(\"quantization.approach\"))\n",
    "calib_dataloader = trainer.get_train_dataloader() if quant_approach == IncQuantizationMode.STATIC else None\n",
    "\n",
    "quantizer = IncQuantizer(\n",
    "            q8_config, \n",
    "            eval_func=eval_func, \n",
    "            train_func=train_func, \n",
    "            calib_dataloader=calib_dataloader\n",
    "        )\n",
    "\n",
    "optimizer = IncOptimizer(\n",
    "    model_fp32,\n",
    "    quantizer=quantizer,\n",
    "    one_shot_optimization=True,\n",
    "    eval_func=eval_func,\n",
    "    train_func=train_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab7eae-40de-4708-a1eb-24905caf5e1f",
   "metadata": {},
   "source": [
    "We can then go ahead and run quantization with `optimizer.fit()`. If you have a relatively small test dataset size, it should quantize the model fairly quickly, within a minute or less. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65762f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-24 16:22:57 [INFO] Start sequential pipeline execution.\n",
      "2022-09-24 16:22:57 [INFO] The 0th step being executing is QUANTIZATION.\n",
      "2022-09-24 16:22:57 [INFO] Pass query framework capability elapsed time: 151.37 ms\n",
      "2022-09-24 16:22:58 [INFO] Get FP32 model baseline.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 650\n",
      "  Batch size = 128\n",
      "2022-09-24 16:23:00 [INFO] eval_accuracy: 0.9769230769230769\n",
      "2022-09-24 16:23:00 [INFO] Throughput: 226.698 samples/sec\n",
      "2022-09-24 16:23:00 [INFO] Save tuning history to /home/ubuntu/nlp-hackathon/notebooks/nc_workspace/2022-09-24_16-22-51/./history.snapshot.\n",
      "2022-09-24 16:23:00 [INFO] FP32 baseline is: [Accuracy: 0.9769, Duration (seconds): 2.8895]\n",
      "/home/ubuntu/miniconda3/envs/nlp_hackathon_inference/lib/python3.8/site-packages/torch/ao/quantization/qconfig.py:92: UserWarning: QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead\n",
      "  warnings.warn(\"QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead\")\n",
      "2022-09-24 16:23:00 [INFO] Fx trace of the entire model failed, We will conduct auto quantization\n",
      "/home/ubuntu/miniconda3/envs/nlp_hackathon_inference/lib/python3.8/site-packages/torch/nn/quantized/_reference/modules/utils.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"scale\"], dtype=torch.float, device=device))\n",
      "/home/ubuntu/miniconda3/envs/nlp_hackathon_inference/lib/python3.8/site-packages/torch/nn/quantized/_reference/modules/utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"zero_point\"], dtype=zero_point_dtype, device=device))\n",
      "2022-09-24 16:23:02 [INFO] |******Mixed Precision Statistics*****|\n",
      "2022-09-24 16:23:02 [INFO] +----------------+----------+---------+\n",
      "2022-09-24 16:23:02 [INFO] |    Op Type     |  Total   |   INT8  |\n",
      "2022-09-24 16:23:02 [INFO] +----------------+----------+---------+\n",
      "2022-09-24 16:23:02 [INFO] |   Embedding    |    2     |    2    |\n",
      "2022-09-24 16:23:02 [INFO] |     Linear     |    38    |    38   |\n",
      "2022-09-24 16:23:02 [INFO] +----------------+----------+---------+\n",
      "2022-09-24 16:23:02 [INFO] Pass quantize model elapsed time: 1281.26 ms\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 650\n",
      "  Batch size = 128\n",
      "2022-09-24 16:23:04 [INFO] eval_accuracy: 0.98\n",
      "2022-09-24 16:23:04 [INFO] Throughput: 344.279 samples/sec\n",
      "2022-09-24 16:23:04 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.9800|0.9769, Duration (seconds) (int8|fp32): 1.9108|2.8895], Best tune result is: [Accuracy: 0.9800, Duration (seconds): 1.9108]\n",
      "2022-09-24 16:23:04 [INFO] |**********************Tune Result Statistics**********************|\n",
      "2022-09-24 16:23:04 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-09-24 16:23:04 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
      "2022-09-24 16:23:04 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-09-24 16:23:04 [INFO] |      Accuracy      | 0.9769   |    0.9800     |     0.9800       |\n",
      "2022-09-24 16:23:04 [INFO] | Duration (seconds) | 2.8895   |    1.9108     |     1.9108       |\n",
      "2022-09-24 16:23:04 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2022-09-24 16:23:04 [INFO] Save tuning history to /home/ubuntu/nlp-hackathon/notebooks/nc_workspace/2022-09-24_16-22-51/./history.snapshot.\n",
      "2022-09-24 16:23:04 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2022-09-24 16:23:04 [INFO] Save deploy yaml to /home/ubuntu/nlp-hackathon/notebooks/nc_workspace/2022-09-24_16-22-51/deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "agent = optimizer.get_agent()\n",
    "optimized_model = optimizer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16451da5-99d4-4a31-bc85-a127f0bfec3a",
   "metadata": {},
   "source": [
    "And now that we have an optimized model, we can run evaluation and then compare it to the baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "718b7f30-13f1-4909-9d8a-cfd8673df0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 650\n",
      "  Batch size = 128\n",
      "2022-09-24 16:23:06 [INFO] eval_accuracy: 0.98\n",
      "2022-09-24 16:23:06 [INFO] Throughput: 337.179 samples/sec\n"
     ]
    }
   ],
   "source": [
    "result_optimized_model = take_eval_steps(optimized_model, trainer, metric_name, save_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f03f0-e001-4dda-a86f-215866770f7f",
   "metadata": {},
   "source": [
    "We now can save the quantized model, and have both the FP32 and the newly quantized INT8 model available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "526c2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../output_quantized/config.json\n",
      "2022-09-24 16:23:06 [INFO] Model weights saved to ../output_quantized\n",
      "2022-09-24 16:23:06 [INFO] Optimized model with eval_accuracy of 0.98 and sparsity of 0.86% saved to: ../output_quantized. Original model had an eval_accuracy of 0.9769230769230769.\n"
     ]
    }
   ],
   "source": [
    "# Save the resulting model and its corresponding configuration in the given directory\n",
    "optimizer.save_pretrained(training_args.output_dir)\n",
    "# Compute the model's sparsity\n",
    "sparsity = optimizer.get_sparsity()\n",
    "logger.info(\n",
    "    f\"Optimized model with {metric_name} of {result_optimized_model} and sparsity of {round(sparsity, 2)}% \"\n",
    "    f\"saved to: {training_args.output_dir}. Original model had an {metric_name} of {result_baseline_model}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a6c9b",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background:linear-gradient(90deg, navy, #dc98ff, #251cab);overflow:hidden\">5. Model Inference on Intel Gen. 3 Xeon CPU</div></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd484f",
   "metadata": {},
   "source": [
    "Now that we have a quantized model, let's test the inference on a test dataset to make sure that we have not lost significantly in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba3e6f-c195-45d9-a9e9-609ecc1460c8",
   "metadata": {},
   "source": [
    "In the first code snippet, I am loading the FP32 model, putting it on the CPU device, and then running inference on a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfcfa266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../models/checkpoint-2000/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../models/checkpoint-2000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ../models/checkpoint-2000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 650\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************** Evaluation below************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_accuracy           =     0.9769\n",
      "  eval_loss               =     0.1376\n",
      "  eval_runtime            = 0:00:02.68\n",
      "  eval_samples            =        650\n",
      "  eval_samples_per_second =    242.421\n",
      "  eval_steps_per_second   =      2.238\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run() #new model, so the same parameters cannot be used as in the last mlflow run.\n",
    "\n",
    "# model loading: load in the FP32 model\n",
    "output_model_folder = './models/checkpoint-2000' #this may change depending on where you saved your model\n",
    "model_fp32 = DistilBertForSequenceClassification.from_pretrained(output_model_folder) \n",
    "model_fp32.to(device)\n",
    "print('')\n",
    "\n",
    "trainer_fp32 = Trainer(\n",
    "    model=model_fp32,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,             # evaluation dataset\n",
    "    compute_metrics = compute_metrics\n",
    "    )\n",
    "print(\"**************** Evaluation below************\")\n",
    "metrics = trainer_fp32.evaluate()\n",
    "metrics[\"eval_samples\"] = len(test_dataset)\n",
    "trainer_fp32.log_metrics(\"eval\", metrics)\n",
    "trainer_fp32.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c69afe-a7ba-4cb9-8337-d1f5354262c4",
   "metadata": {},
   "source": [
    "In the second code snippet, I am loading the INT8 model, and running inference on the same test set, to compare it to the FP32 model. What you should see is a very similar accuracy/F1 score, but the INT8 model should show that it can handle more samples per second than the FP32 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddcf1c5f-3c8d-4185-9d5d-e1ca5d98d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../output_quantized/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"../output_quantized\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"int8\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file ../output_quantized/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"../models/checkpoint-2000\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"int8\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../output_quantized/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ../output_quantized.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "/home/ubuntu/miniconda3/envs/nlp_hackathon_inference/lib/python3.8/site-packages/torch/ao/quantization/qconfig.py:92: UserWarning: QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead\n",
      "  warnings.warn(\"QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead\")\n",
      "/home/ubuntu/miniconda3/envs/nlp_hackathon_inference/lib/python3.8/site-packages/torch/nn/quantized/_reference/modules/utils.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"scale\"], dtype=torch.float, device=device))\n",
      "/home/ubuntu/miniconda3/envs/nlp_hackathon_inference/lib/python3.8/site-packages/torch/nn/quantized/_reference/modules/utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"zero_point\"], dtype=zero_point_dtype, device=device))\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 650\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************** Evaluation below************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_accuracy           =       0.98\n",
      "  eval_loss               =      0.106\n",
      "  eval_runtime            = 0:00:01.80\n",
      "  eval_samples            =        650\n",
      "  eval_samples_per_second =    359.668\n",
      "  eval_steps_per_second   =       3.32\n"
     ]
    }
   ],
   "source": [
    "#load in the quantized INT8 model.\n",
    "quantized_model_folder = './output_quantized/'\n",
    "model_int8 = IncQuantizedModelForSequenceClassification.from_pretrained(training_args.output_dir)\n",
    "model_int8.to(device)\n",
    "print('')\n",
    "\n",
    "trainer_int8 = Trainer(\n",
    "    model=model_int8,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset,             # evaluation dataset\n",
    "    compute_metrics = compute_metrics\n",
    "    )\n",
    "\n",
    "print(\"**************** Evaluation below************\")\n",
    "metrics = trainer_int8.evaluate()\n",
    "metrics[\"eval_samples\"] = len(test_dataset)\n",
    "trainer_int8.log_metrics(\"eval\", metrics)\n",
    "trainer_int8.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec2014-8b56-4d5e-8659-105ca66d0eb0",
   "metadata": {},
   "source": [
    "Let's run the FP32 and INT8 models on the unseen test dataset (10,000 rows). <br>\n",
    "Batch size was set at 1000, so should be 10 passes through the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33032231-f2f1-44c7-9b41-d134223e3986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nlp-hackathon/notebooks/../src/nlpload.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_time=56.26547646522522\n"
     ]
    }
   ],
   "source": [
    "y_true,y_preds_raw,infer_time = evaluate.nlp_evaluate(model_fp32,torch_dataloader,device) #function in ../src/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8a3b782-ca45-4a08-b15d-9910321e3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_class_from_preds(y_preds_raw):\n",
    "    pred_y_class = []\n",
    "    start_time = time.time()\n",
    "    for y in tqdm(y_preds_raw):\n",
    "        pred_y_class.extend(y.tolist())\n",
    "    print(f'time = {time.time()-start_time}')\n",
    "    return pred_y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4484b086-0558-4f3a-9520-11e216fb1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 47500.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0.0021653175354003906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_pred_y = extract_class_from_preds(y_preds_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a41111a2-8fb0-4996-9f63-ab57b969f94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ee30f34-2c2a-4ae2-bfe3-2c84e85ba756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_time=43.54761600494385\n"
     ]
    }
   ],
   "source": [
    "y_true_2,y_preds_raw_2,infer_time_2 = evaluate.nlp_evaluate(model_int8,torch_dataloader,device) #function in ../src/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66df8b27-f9a5-4830-8d3e-e1cd2a067cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 38800.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0.0017940998077392578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_pred_y_2 = extract_class_from_preds(y_preds_raw_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1979e5ac-8739-409f-bab0-516951bb2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_10k = pd.read_csv(csvpath)\n",
    "true_false_list = [bool(y) for y in output_pred_y_2]\n",
    "hdf_10k['humor'] = true_false_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6a2ff86-8100-4934-a62b-03b0eb135837",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = 'Team_NLP0'\n",
    "output_csvpath = f'../output_quantized/{team_name}.csv' #csv must be named with your team name. Eg., \"Team_NLP0.csv\"\n",
    "hdf_10k.to_csv(output_csvpath,index=False)  #outputting CSV with humor prediction label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc3bbf",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;;background:linear-gradient(90deg, navy, #dc98ff, #251cab)\">6. Summary</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d75e2",
   "metadata": {},
   "source": [
    "A summary of learnings in this notebook:\n",
    "\n",
    "- Loaded, split train-val-test, and tokenized the humor text data\n",
    "- Quantized our output model from FP32 to INT8 for faster inference speed\n",
    "- Evaluated the quantized model inference speed on a small test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e119ad-0776-4db3-9c3f-8adb1e696bd0",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;;background:linear-gradient(90deg, navy, #dc98ff, #251cab)\">7. References</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d870176",
   "metadata": {},
   "source": [
    "- Habana Gaudi instance: \n",
    "    - https://aws.amazon.com/ec2/instance-types/dl1/\n",
    "    - https://habana.ai/training/gaudi/\n",
    "    - https://www.intel.com/content/www/us/en/developer/articles/technical/get-started-habana-gaudi-deep-learning-training.html#gs.9p3p1b\n",
    "- Humor problem statement: Jain, Manan. \"Humor Detection.\" (2017). https://core.ac.uk/download/pdf/234824434.pdf\n",
    "- Optimum Habana GitHub: https://github.com/huggingface/optimum-habana\n",
    "- Optimum Intel GitHub: https://github.com/huggingface/optimum-intel\n",
    "- DistilBERT Model: Sanh, Victor, et al. \"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.\" arXiv preprint arXiv: https://arxiv.org/abs/1910.01108 (2019).\n",
    "\t- Model on Huggingface:  https://huggingface.co/distilbert-base-uncased\n",
    "- Pruning and Distillation: https://www.intel.com/content/www/us/en/developer/articles/technical/compression-and-acceleration-of-high-dimensional-neural-networks.html\n",
    "- Intel¬Æ Xeon¬Æ:\n",
    "    - 3rd Gen: https://ark.intel.com/content/www/us/en/ark/products/series/204098/3rd-generation-intel-xeon-scalable-processors.html\n",
    "    - 4th Gen: https://edc.intel.com/content/www/us/en/products/performance/benchmarks/architecture-day-2021/?r=1156525610\n",
    "    \n",
    "## <span style=\"padding:0px;color:#251cab;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">Notices & Disclaimers</span>\n",
    "Intel optimizations, for Intel compilers or other products, may not optimize to the same degree for non-Intel products.  \n",
    "Performance varies by use, configuration and other factors. Learn more on the Performance Index site.   \n",
    "Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure.   \n",
    "Your costs and results may vary.   \n",
    "Intel technologies may require enabled hardware, software or service activation.  \n",
    "&copy; Intel Corporation.  Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and brands may be claimed as the property of others.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_hackathon_inference",
   "language": "python",
   "name": "nlp_hackathon_inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1fa0ca49ba07ce2a92f82f00ba7dde46659924fa31a5ff32fcd0325c256a91b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
